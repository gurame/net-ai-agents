Concepts
--------
LLM = It is a prediction model in essence, it predicts the next word in a sequence based on the words that came before it.
Pretrained = All the heavy lifting is done, the models have already been trained on massive datasets
Transformers = It is specific neural network architecture that is particularly well-suited for processing sequential data, such as text. 
    It was introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017.
Neural Networks = They are excellent spotting patterns in data, making them ideal for tasks like image and speech recognition, as well as natural language processing.
Probability Distributions = This is the output of a Neural Network, which gives the likelihood of each possible next word in a sequence.
Attention Mechanism = It is a mechanism that allows the model to focus on different parts of the input sequence when making predictions.

Prompt Engineering = It is the skill of being able to use language to give the model some context and make it response in the way you want it to.
System Prompt = It sets the behavior and context for the AI, guiding how it should respond to user inputs.
    LLMs like ChatGPT are actually trained to put extra weight on the system prompt, so it has a significant influence on the model's behavior.
User Prompt = It is the input provided by the user, which the AI responds to based on the context set by the system prompt.
Few-shot Prompting = It involves providing the model with a few examples of the desired input-output behavior within the prompt itself.
    This helps the model understand the task better and generate more accurate responses.
Chain-of-Thought Prompting = It is a technique where the model is guided to generate intermediate reasoning steps before arriving at a final answer.
    This can help improve the model's performance on complex tasks that require multi-step reasoning.
    
Context Window = It refers to the maximum amount of text (in tokens) that the model can consider at once when generating a response.
    If the input exceeds this limit, the model may lose important context, leading to less accurate or relevant responses.
    LLMs are stateless, meaning they do not retain memory of past interactions unless that information is included in the current prompt.
    
MTP = It stands for Multi-Turn Prompting, which involves structuring prompts to include multiple interactions or turns in a conversation.
    This technique helps maintain context and coherence in dialogues with the model.
    MTP can be particularly useful for applications like chatbots or virtual assistants, where maintaining the flow of conversation is crucial.
    
MTP Spec = It is a structured format for organizing multi-turn prompts, ensuring clarity and coherence in the interactions with the model.
    The MTP Spec typically includes sections for system prompts, user prompts, and any necessary context or instructions.
    By following a standardized MTP Spec, developers can create more effective and consistent interactions with LLMs.
    
Completion Models = These models generate text by predicting the next word in a sequence based on the preceding words.
    They are typically used for tasks like text generation, summarization, and translation.
    
Reasoning Models = These models are designed to perform tasks that require logical thinking and problem-solving.
    They can handle more complex queries and provide more accurate answers by simulating human-like reasoning processes.
    
Tool Calling = It refers to the ability of LLMs to interact with external tools or APIs to enhance their functionality.
    This can include tasks like retrieving real-time information, performing calculations, or accessing databases.
    By integrating tool calling capabilities, LLMs can provide more accurate and contextually relevant responses.
    
Agent Loop = It is a framework that allows LLMs to interact with external tools and APIs in a structured manner.
    The agent loop typically involves the following steps:
        1. The model receives a user prompt and generates an initial response.
        2. If the response requires additional information or actions, the model identifies the necessary tools or APIs to call.
        3. The model makes the appropriate tool calls and retrieves the required data.
        4. The model integrates the retrieved data into its response and generates a final output for the user.
    This iterative process allows LLMs to perform more complex tasks by leveraging external resources.
    
Reasoning = It is the cognitive process of analyzing information, drawing conclusions, and making decisions based on evidence and logic.
    In the context of LLMs, reasoning involves the model's ability to understand complex queries, identify relevant information, and generate coherent and accurate responses.
    Effective reasoning in LLMs can be enhanced through techniques like chain-of-thought prompting and tool calling, which help the model break down complex tasks into manageable steps.
    
Memory Management = It refers to the strategies and techniques used to handle and utilize information effectively within LLMs.
    Since LLMs are stateless and have a limited context window, memory management is crucial for maintaining coherence and relevance in responses.
    Techniques for memory management include:
        1. Contextual Summarization: Summarizing previous interactions to fit within the context window while retaining essential information.
        2. External Memory Storage: Using databases or other storage solutions to keep track of past interactions and retrieve relevant information when needed.
        3. Dynamic Prompting: Adjusting prompts based on the current context and user needs to ensure the model has access to the most relevant information.
    By implementing effective memory management strategies, developers can enhance the performance and user experience of applications built on LLMs.
    
Embeddings = They are numerical representations of words or phrases in a high-dimensional space, capturing semantic relationships between them.
    In the context of LLMs, embeddings are used to represent text data in a way that the model can understand and process.
    Embeddings can be generated using various techniques, such as Word2Vec, GloVe, or transformer-based models like BERT.
    By leveraging embeddings, LLMs can perform tasks like similarity search, clustering, and classification more effectively.